{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e621921-a910-4361-9a2a-c451f84211cd",
   "metadata": {},
   "source": [
    "## grab all MERRA2 fields for the bounding box +/- 5 degrees and write out \n",
    "\n",
    "To help us composite gridded quantities by distance from the CWV=55mm boundary contour, with a `groupby().mean()` operation, we need to construct a new distance field on the MERRA2 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31300c9e-fd42-4360-87e4-e108dc43f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import geopandas as gp\n",
    "\n",
    "from shapely.geometry import Point\n",
    "import pyproj\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a106d12-9386-4aed-8d67-9c6a455a2102",
   "metadata": {},
   "source": [
    "# Open MERRA2 files and combine all the wanted fields \n",
    "### (2D hourly collections on goldsmr4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "699254d5-ab83-457c-92a8-df70293ef72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2diag = xr.open_dataset('https://goldsmr4.gesdisc.eosdis.nasa.gov/dods/M2T1NXINT')\n",
    "d2flux = xr.open_dataset('https://goldsmr4.gesdisc.eosdis.nasa.gov/dods/M2T1NXFLX')\n",
    "d2slv = xr.open_dataset('https://goldsmr4.gesdisc.eosdis.nasa.gov/dods/M2T1NXSLV')\n",
    "\n",
    "var_diag = ['dqvdt_phy','dqvdt_dyn','dqvdt_ana','uflxqv','vflxqv','swnettoa']\n",
    "var_flux = ['eflux','evap','hflux','preccon','precanv','prectot','prevtot','prectotcorr']\n",
    "var_slv = [ 't2m','ts','t850','t500','q850','q500','slp','tqv','tqi','tql',  \\\n",
    "           'disph','tropt' ] # dummy ones to re-use as distance and direction\n",
    "\n",
    "d2super = d2diag[ var_diag ].merge(d2flux[ var_flux ]).merge(d2slv[ var_slv ])\\\n",
    "        .rename({\"disph\":\"distance\"}).rename({\"tropt\":\"dir_from_centroid\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53036a8b-6644-4f4d-9fed-7ee8550b4517",
   "metadata": {},
   "source": [
    "## Read all lakes in 2014-2018 \n",
    "#### lakes dataframe was \"improved\" in LakeCaseStudy.ipynb with firsttime added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1ec77ac-3dd7-4f5b-8d12-0094f0c43289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ccvls_stats_2014-2018.improved.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd27b987-4769-471f-bd21-225888ca2013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equatorcases = df[ abs(df.coastlat) < 10 ] # 162 of them exceeding 1 day \n",
    "eq7cases = equatorcases[ equatorcases.dur_days > 6 ]  # 39 of them exceeding a week (more than 6 in days) \n",
    "eq7cases.maxarea.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf048b61-b26b-4e1c-8ee9-f55ff385f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for icase in range(len(eq7cases)): \n",
    "\n",
    "# Continue from where it last crashed/ended:\n",
    "    if(icase < 8): \n",
    "        continue\n",
    "\n",
    "# Select case and move forward\n",
    "    case = eq7cases.iloc[icase]\n",
    "    filename = case.filename\n",
    "    print('case ',filename, icase)\n",
    "\n",
    "    gdf = gp.read_file('GEOJSONS/'+filename)\n",
    "    gdf.set_crs(epsg = \"4326\", inplace = True)\n",
    "    bounds = gdf.bounds\n",
    "    bounds.minx.min(), bounds.maxx.max(), bounds.miny.min(), bounds.maxy.max(),\n",
    "\n",
    "    d2 = d2super.sel( lat =slice(bounds.miny.min()-5, bounds.maxy.max()+5),\n",
    "                  lon =slice(bounds.minx.min()-5, bounds.maxx.max()+5), \n",
    "                  time=slice(gdf.time.min(),gdf.time.max())             )\n",
    "\n",
    "    # Put a dummy distance field, initially large so min operator will replace it later \n",
    "    d2.distance.values = d2.distance.values*0 +999.\n",
    "    d2['distance'].attrs['long_name'] = 'distance outside nearest lake (negative for interiors)'\n",
    "    d2['distance'].attrs['units'] = 'degrees lat/lon'\n",
    "    d2['dir_from_centroid'].attrs['long_name'] = 'navigation angle from centroid of nearest lake'\n",
    "    d2['dir_from_centroid'].attrs['units'] = 'nav.degrees'\n",
    "\n",
    "# Make an array of Points that are the gridpoints or MERRA2\n",
    "    lat2d = d2.lat.values[:,None]   + d2.lon.values*0\n",
    "    lon2d = d2.lat.values[:,None]*0 + d2.lon.values\n",
    "    points = gp.GeoSeries( [Point(x, y) for x, y in zip(lon2d.ravel(),lat2d.ravel()) ] )\\\n",
    "            .set_crs(epsg = \"4326\", inplace = True)\n",
    "    \n",
    "    for i in range(gdf.time.size):     # the number of rows in the geoseries (polygons), not # of times\n",
    "                \n",
    "# Compute distance and direction fields:\n",
    "# DIST: must compute distance to polygon BOUNDARY to get nonzero values inside it \n",
    "        dist = points.distance(gdf.geometry[i].boundary).values.reshape(len(d2.lat),len(d2.lon))\n",
    "        isin = points.within(gdf.geometry[i]).values.reshape(len(d2.lat),len(d2.lon))\n",
    "        dist *= (-2)*(isin-0.5)  # make SIGNED distance from boundary, positive is exterior \n",
    "\n",
    "# DIR: use geodesic method imported above. \n",
    "# Direction to centroid of WHOLE LAKE (multi-polygon in general) at this time\n",
    "        agg_lake_at_time = gdf.geometry[ np.where(gdf.time == gdf.time[i])[0] ].unary_union\n",
    "        centlon = agg_lake_at_time.centroid.x\n",
    "        centlat = agg_lake_at_time.centroid.y\n",
    "    \n",
    "    #Careful, use j not i in stupid explicit loop, since geodesic takes scalar only \n",
    "        dir_to = []\n",
    "        for j in range(len(lon2d.ravel())): \n",
    "            fwd_az,back_az,d = geodesic.inv(lon2d.ravel()[j], lat2d.ravel()[j], [centlon], [centlat])\n",
    "            dir_to.append(fwd_az+180)\n",
    "        dir_to = np.array(dir_to).reshape(len(d2.lat),len(d2.lon))\n",
    "    \n",
    "# if dist is less than previous distance in the dataset, replace that previous\n",
    "        oldd = d2.sel(time = gdf.time[i],method='nearest').distance\n",
    "        mindist = np.minimum(dist,oldd)\n",
    "    # closer = np.where(dist < oldd) # a spatial index of where, couldn't make it work\n",
    "    \n",
    "# Here's a tricky thing, assigning values to the dataset - need to use index instead of sel\n",
    "# sel cannot be used for assignment of values, only extraction of them. \n",
    "# idx of the time in d2 corresponding to this geometry object in the gdf\n",
    "        idx = d2.indexes[\"time\"].get_indexer([gdf.time[i]],method='nearest')[0]\n",
    "\n",
    "# Assign the new mindist field at this time, & dir where (dist < oldd) for this object\n",
    "        d2[\"distance\"][idx] = mindist\n",
    "    \n",
    "# Assigning direction is even trickier. I can't seem to use \"closer\" \n",
    "# to reassign direction only in places where dist < oldd, such that \n",
    "# the direction field would be from the centroid of the CLOSEST polgon. \n",
    "# Instead, let's just assign direction from the centroid of the whole \n",
    "# unary union of all the individual polygons \n",
    "\n",
    "        d2[\"dir_from_centroid\"][idx] = dir_to\n",
    "\n",
    "#  these 1-hour averages are 30 minutes offset from tqv, argh. \n",
    "# just manually spray the mindist values to the next earlier time level too \n",
    "        try: \n",
    "            d2[\"distance\"][idx-1] = mindist\n",
    "            d2[\"dir_from_centroid\"][idx-1] = dir_to\n",
    "\n",
    "        except: \n",
    "            print('end of loop issue (first time in time-backward sequence over geometries)')\n",
    "\n",
    "        print('Writing outputs')\n",
    "        d2.to_netcdf('../../Library/CloudStorage/Box-Box/VaporLakes/data/LAKEBYLAKE/'+\\\n",
    "             filename[0:-8]+'.M2_all2Dfields.nc')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b2ca8-47c5-426c-8d05-a24c4e191c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b799ec-2a43-4d20-a99e-5beb70a149cf",
   "metadata": {},
   "source": [
    "# Consider making IDV bundle for it \n",
    "### by relacing filename inside .xidv of a template bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "349ad369-03d1-436f-a65d-11fc12fc756f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017_05_06_20_lat3p790S.M2_all2Dfields.nc\n",
      "2017_05_08_05_lat1p160N.M2_all2Dfields.nc\n",
      "2017_05_09_01_lat3p355S.M2_all2Dfields.nc\n",
      "2017_05_28_11_lat5p291S.IMERG_30minute.nc\n",
      "2017_05_28_11_lat5p291S.M2_all2Dfields.nc\n",
      "2017_05_28_11_lat5p291S.M2_all2Dfields_myfirst.nc\n",
      "2017_05_28_11_lat5p291S.M2_composite.nc\n",
      "2017_12_01_11_lat9p845S.M2_all2Dfields.nc\n",
      "2018_03_19_12_lat9p318S.M2_all2Dfields.nc\n",
      "2018_10_22_15_lat9p781N.M2_all2Dfields.nc\n",
      "2018_11_16_21_lat8p412S.M2_all2Dfields.nc\n",
      "2018_11_20_11_lat4p616S.M2_all2Dfields.nc\n",
      "\u001b[34mdirection_field_wrong_fixable\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls ../../Library/CloudStorage/Box-Box/VaporLakes/data/LAKEBYLAKE/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b213d704-6ee6-4991-937d-f0131a2dd7db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9aaf7910-e970-4ef2-8921-40d906864001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inplace replacement works \n",
    "\n",
    "!cp ../../Library/CloudStorage/Box-Box/VaporLakes/Landfalling_Lake_Event_Displays.xidv test.txt\n",
    "\n",
    "!sed -i '' \"s/2017_05_28_11_lat5p291S.M2_all2Dfields_myfirst/2018_11_20_11_lat4p616S.M2_all2Dfields.nc]/g\" test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ab1d793-ea9f-4b41-b55d-520ab4614fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                <string><![CDATA[/Users/brianmapes/Library/CloudStorage/Box-Box/VaporLakes/data/LAKEBYLAKE/2018_11_20_11_lat4p616S.M2_all2Dfields.nc].nc]]></string>\n",
      "                        <string><![CDATA[/Users/bem/Box/VaporLakes/data/LAKEBYLAKE/2018_11_20_11_lat4p616S.M2_all2Dfields.nc].nc]]></string>\n",
      "                                        <string><![CDATA[/Users/bem/Box/VaporLakes/data/LAKEBYLAKE/2018_11_20_11_lat4p616S.M2_all2Dfields.nc].nc]]></string>\n"
     ]
    }
   ],
   "source": [
    "!grep Box test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f24ad73-b2aa-4468-9a68-c9c971b81c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d511f484-6201-480f-97b5-c990c949e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '../../Library/CloudStorage/Box-Box/VaporLakes/Landfalling_Lake_Event_Displays.xidv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3141453-6195-4d26-b9f9-038aca6b2ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
